{"3-ways-to-generate-distractors-wrong-choices-for-mcqs-using-natural-language-processing-d52477a56812": "Distractors are the wrong answers in a multiple-choice question. For example, if a given multiple choice question has the game Cricket as the correct answer then we need to generate wrong choices (distractors) like Football, Golf, Ultimate Frisbee, etc. Multiple-choice questions are the most popular assessment questions created whether it is for a school test or a graduate competitive exam. Coming up with efficient distractors for a given question is a very time-consuming process for question authors/teachers. Given the increased volume of workload on teachers/assessment creators due to Covid, it would be very helpful if we can create an automated system to create these distractors. In this article, we will see how we can use Natural Language processing techniques to solve this practical problem. In particular, we will see how we can generate distractors using 3 different algorithms - WordNetConceptNetSense2Vec Distractors shouldn\u2019t be too similar or too different. Distractors should be homogeneous in content. Eg: Pink, Blue, Watch, Green makes it obvious that Watch might be the answer.Distractors should be mutually exclusive. So shouldn\u2019t include synonyms. Let\u2019s get started. WordNet\u00ae is a large lexical database of English.Similar to thesaurus but captures even broader relationships between words.WordNet is also free and publicly available for download.WordNet labels the semantic relations among words. Eg: Synonyms \u2014 Car and AutomobileWordNet also captures the different senses of a word. Eg: Mouse could mean an animal or computer mouse. Generating distractors using Wordnet If we have a sentence \u201c The bat flew into the jungle and landed on a tree\u201d and a keyword \u201cbat\u201d, we automatically know that here we are talking about the mammal bat that has wings, not a cricket bat or baseball bat. Although we humans are good at it, the algorithms are not very good at distinguishing one from the other. This is called word sense disambiguation (WSD). In Wordnet \u201cbat\u201d may have several senses (meanings) one for a cricket bat, one for a flying mammal, etc. Word sense disambiguation using NLP is a separate topic in itself so for this tutorial, we assume that the exact \u201csense\u201d is manually identified by the user or the top sense is chosen automatically. Let\u2019s say we get a word like \u201cRed\u201d and identify its sense, we then go to its hypernym using Wordnet. A hypernym is a higher level category for a given word. In our example, color is the hypernym for Red. Then we go to find all hyponyms (sub-categories) of color which might be Purple, Blue, Green, etc all belonging to the color group. So we can use Purple, Blue, Green, as distractors (wrong answer choices) for the given MCQ which has Red as the correct answer. Purple, Blue, Green are also called as Co-Hyponyms of Red. So using Wordnet, extracting co-hyponyms of a given word gets us the distractors of that word. So in the image below, the distractors of Red are Blue, Green, and Purple. Wordnet Code Let\u2019s start by installing NLTK Import wordnet Get all distractors for the word \u201clion\u201d. Here we are extracting the first sense of the word Lion and extracting co-hyponyms of the word lion as distractors. The output from above : Similarly, for the word cricket, which has two senses (one for insect and one for the game) we get different distractors for each depending on which sense we use. Output: Conceptnet is a free multilingual knowledge graph.It includes knowledge from crowdsourced resources and expert-created resources.Similar to WordNet, Conceptnet labels the semantic relationships among words. They are more detailed than Wordnet. Eg: is a, part of, made of, similar to, is used for, etc relations among words is captured. Generating distractors using Conceptnet Conceptnet is good for generating distractors for things like locations, items, etc which have a \u201cPartof\u201d relationship. For example, a state like \u201cCalifornia\u201d could be part of the United States. A \u201ckitchen\u201d could be part of a house etc. Conceptnet doesn\u2019t have a provision to disambiguate between different word senses as we discussed above with the example of the bat. Hence we need to go ahead with whatever sense Conceptnet gives us when we query with a given word. Let\u2019s see how we can use Conceptnet in our use case. We don\u2019t need to install anything because we can use the Conceptnet API directly. Note that there is an hourly API rate limit so beware of it. Given a word like \u201cCalifornia\u201d we query Conceptnet with it and retrieve all the words that share \u201cPartof\u201d relationship with it. In our example, \u201cCalifornia\u201d is part of the \u201cUnited States\u201d. Now we go to \u201cUnited States\u201d and see what other things does it share a \u201cPartof\u201d relationship with. That would be other states like \u201cTexas\u201d, \u201cArizona\u201d, \u201cSeattle\u2019 etc. Similar to Wordnet we extract co-hyponyms using \u201cpartof\u201d relationship for our query word \u201cCalifornia\u201d and we fetch the distractors \u201cTexas\u201d, \u201cArizona\u201d etc. Code for Conceptnet Output: Unlike Wordnet and Conceptnet the relationships between words in Sense2vec are not human-curated but automatically generated from a text corpus. A neural network algorithm is trained with millions of sentences to predict a focus word given other words or predict surrounding words given a focus word. Through this, we generate a fixed size vector or array representation for each word. We call these word vectors. The interesting fact is that these vectors capture associations among different kinds of words. For example, words of a similar kind in the real world fall closer in the vector space. Also, relationships among different words are preserved. If we take the word vector of King, subtract the vector of Man and add the vector of Woman, we get back the vector of Queen. The real-world relationship of \u201cwhat king is to a man, a queen is the same to woman\u201d is preserved. King \u2014 Man + Woman = Queen Sense2vec is trained on Reddit comments. Noun phrases and named entities are annotated during training so multiword phrases like \u201cnatural language processing\u201d also have an entry as opposed to some word vector algorithms which are trained with only single words. We will use 2015 trained Reddit vectors as opposed to 2019 as the results were slightly better in my experimentation. Code for Sense2vec Install sense2vec from pip Download and unzip Sense2vec vectors Load sense2vec vectors from the unzipped folder on disk Get distractors for a given word. Eg: Natural Language processing Output: I launched a very interesting Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it, here is the link. Hope you enjoyed how we solved a real-world problem of generating distractors (wrong choices) in MCQs using NLP. If you would like a detailed video of the above blog as well as know more about the \u201cQuestion generation using NLP\u201d course that I am launching soon, please watch this video. If you would like to get updates on more practical AI projects feel free to follow me on Linkedin or Twitter. Happy Learning!", "questgen-an-open-source-nlp-library-for-question-generation-algorithms-1e18067fcdc6": "Question answering is a very popular task in Natural language processing but question generation is novel and hasn\u2019t been explored much yet. If you want to try a live demo of question generation in action, please visit https://questgen.ai/ Question generation has a lot of use cases with the most prominent one being the ability to generate quick assessments from any given content. It would help school teachers in generating worksheets from any given chapter quickly and decrease their work burden during Covid-19. I along with two other awesome interns Parth Chokhra and Vaibhav Tiwari built an easy-to-use, open-source library to advance the research in question generation using the state-of-the-art T5 transformer model from Hugging Face library. The currently supported question generation capabilities of the library are MCQs, Yes/No questions, FAQs, Paraphrasing, and Question Answering. Without further delay let\u2019s dive into the details. All the code and an easy to use Google Colab can be found here : ramsrigouthamg/Questgen.ai github.com Install the libraries: Download and extract zip of Sense2vec word vectors that are used for the generation of multiple choices. 1. Generate boolean (Yes/No) Questions Output: 2. Generate MCQ Questions Output: 3. Generate FAQ Questions Output : 4. Paraphrasing Questions Output: 5. Question Answering (Simple) Output: 6. Question Answering (Boolean) Output: Question Generation using NLP Course I launched a practical Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it here is the link. NLP models used For maintaining meaningfulness in Questions, Questgen uses Three T5 models. One for Boolean Question generation, one for MCQs, FAQs, Paraphrasing, and one for answer generation. Quora Question pairs, BoolQ, SQUAD, and MSMarco are the datasets used for training. Here are a few of my previous articles that would explain how to build these question generation models from scratch - Generate boolean (yes/no) questions from any content using the T5 transformer model Paraphrase any question using the T5 transformer model Happy learning!", "high-quality-sentence-paraphraser-using-transformers-in-nlp-c33f4482856f": "Input The input to our program will be any English sentence - Output The output will be paraphrased version of the same sentence. Paraphrasing a sentence means, you create a new sentence that expresses the same meaning using a different choice of words. From rewriting your old Social Media posts to college essays to augmenting the dataset when you don't have many examples for your text classification model, there are several use-cases for a paraphraser. As part of building Questgen.ai we created a custom dataset filtered on top of ParaNMT to preserve only diverse high-quality paraphrases. Diverse here means that pairs of sentences are selected such that there is a significant difference in word order or at least the paraphrased output differs by multiple word changes. If you prefer an easy-to-use Google Colab Notebook, it can be found at Questgen\u2019s Github Repo. 1. Installation 2. Running the code We will use the pre-trained model uploaded to the HuggingFace Transformers library hub to run the paraphraser. We will use diverse beam search decoding strategy that gives best results for paraphrases output. More examples can be found in the Google Colab demo mentioned above. The output from the above code is: Original: Once, a group of frogs was roaming around the forest in search of water.paraphrasedoutput: A herd of frogs was wandering around the woods in search of water.paraphrasedoutput: A herd of frogs was wandering around the woods in search of water.paraphrasedoutput: A gang of frogs was wandering around the forest in search of water at one time.paraphrasedoutput: A herd of frogs was swaning around the woods in search of water.paraphrasedoutput: A gang of frogs was roaming about the woods in search of water once more. That\u2019s it! You have a high-quality state-of-the-art sentence paraphraser that you can use in your projects. A word of caution is that the paraphraser output is not always perfect so a human-in-the-loop system might be necessary. Happy coding! If you would like to stay connected please feel free to follow me on Twitter and Linkedin", "poor-mans-gpt-3-few-shot-text-generation-with-t5-transformer-51f1b01f843e": "I\u2019m sure most of you have heard about OpenAI\u2019s GPT-3 and its insane text generation capabilities learning from only a few examples. The concept of feeding a model with very little training data and making it learn to do a novel task is called Few-shot learning. A website GPT-3 examples captures all the impressive applications of GPT-3 that the community has come up with, since its release. GPT-3 is shown to generate the whole Frontend code from just a text description of how a website looks like. It is shown to generate a complete marketing copy from just a small brief (description). There are many more impressive applications that you can check out on the website. GPT-3 essentially is a text-to-text transformer model where you show a few examples (few-shot learning) of the input and output text and later it will learn to generate the output text from a given input text. The GPT-3 prompt is as shown below. You enter a few examples (input -> Output) and prompt GPT-3 to fill for an input. But GPT-3 is not opensource and the costs of the API might be very high for your use case. Now being aware of the text-to-text capabilities of T5 Transformer by Google while working on my opensource question generation project Questgen.ai, I decided to push T5 to do the same on an untrained task and see the results. I must say the results are pretty impressive even with a base T5 model by making it learn from just a few (~10) examples. So the task I gave was this \u2014 Input : I gave a few sentence pairs that are false sentences of each other by replacing the main adjective with its opposite word. Eg: The cat is alive => The cat is dead And after training with only (~10 samples) and < 5 mins of training T5 was able to generate impressive results on unseen sentences. Output : Prompt to T5: The sailor was happy and joyful. T5 Generated sentences (picked from top 3 responses through beam search) : The sailor was unhappy.The sailor was sad. Prompt to T5: The tortoise was very slow. T5 Generated sentences (picked from top 3 responses through beam search) : The tortoise was very fast.The tortoise was very quick. Without much further ado let\u2019s look into the code. Coming up with the code was an interesting exploration for me. I had to go through Hugging Face documentation and figure out writing a minimalistic forward pass and backpropagation code using the T5 transformer. Colab Notebook A cleanly organized Google Colab notebook is available here 1.1 Installation Install HuggingFace transformers and check GPU info on Colab. 1.2 Necessary Imports and model download First a few necessary imports from transformers library- Initialize the mode and its tokenizer - Initialize the optimizer \u2014 Here we are mentioning which parameters of the T5 model needs to be updated after calculating the gradients of every parameter w.r.t to the Loss. 1.3 Training data The complete training data (~10 samples ) that is used for our T5 few-shot text generation task. 1.4 Training the model This is the simple loop where we train our T5 model with the samples from above. Here we train for 10 epochs iterating through each sample pair from our training data. We get the loss in each step, calculate gradients (loss. backward) and update the weights (optimizer. step) which is standard for all deep learning algorithms. Most of the effort was in understanding and getting T5 training to this simple loop :) That\u2019s it. Depending on the GPU, the model is trained in 5 mins or less and it is ready for testing with some unseen samples. Testing the model Test sentence: The sailor was happy and joyful. Using beam decoding we get the top 3 sentences generated from the code as - The sailor was unhappyThe sailor was sadThe sailor was happy One more : Test Sentence: The tortoise was very slow. Using beam decoding we get the top 3 sentences generated from the code as - The tortoise was very slowThe tortoise was very fastThe tortoise was very quick As you can see T5 is able to generate a false sentence of a given sentence even if it has not seen those adjectives or sentence words previously in training. If you have come this far, I have a bonus for you :) Find the same code in the Pytorch Lightning format using the latest version of HuggingFace here I launched a very interesting Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it, here is the link. Hope you enjoyed how we explored T5 for few-shot text generation task, just like GPT-3. When I started exploring T5 last year I realized its potential. It can do quite a few text-to-text tasks very efficiently. We released an open-source library Questgen.ai to do question generation in edtech completely trained on T5 for several different tasks. You might want to check it out. If you would like to get updates on more practical AI projects feel free to follow me on Linkedin or Twitter. Happy Learning!", "containerizing-huggingface-transformers-for-gpu-inference-with-docker-and-fastapi-on-aws-d4a83edede2f": "Using GPU within a docker container isn\u2019t straightforward. There shouldn't be any mismatch between CUDA and CuDNN drivers on both the container and host machine to enable seamless communication. The ideal approach is to use NVIDIA container toolkit image in your docker that provides support to automatically recognize GPU drivers on your base machine and pass those same drivers to your Docker container when it runs. You can read more about using Docker in a GPU in this amazing article by the roboflow team. In this article, we will see how to containerize the summarization algorithm from HuggingFace transformers for GPU inference using Docker and FastAPI and deploy it on a single AWS EC2 machine. You can use the same docker container to deploy on container orchestration services like ECS provided by AWS if you want more scalability. Youtube Video If you would like to watch a video version instead, here it is - https://www.youtube.com/watch?v=I3kkQVNuXyc First, we will put together code for a summarizer using HuggingFace transformers. You can take a look at this Colab notebook and run it to see the summarization for yourself. The base code for summarization looks like this: The output from the above code is the summary of the text we passed to it - Now we will see the steps to containerize and deploy this as an API to do GPU inferencing on an AWS EC2 machine. All the code necessary to create a GPU docker container for the summarization algorithm above is present in this Github repo. We will spin up an EC2 GPU machine (g4dn.xlarge), from a base AMI image (Deep Learning Base AMI (Ubuntu 18.04) Version 42.0), create the docker image, and run the image to serve the API for summarization on the same machine. Step1: Login to your AWS console and go to the EC2 dashboard. Step2: Once you are on the dashboard, click on \u201cLaunch instances\u201d on the top right. Step3: Now we are asked to choose an AMI. Enter \u201cDeep Learning Base AMI\u201d in the search bar. Instead of installing CUDA, CuDNN, and Docker ourselves which is hard, we are opting to choose an Amazon machine image (AMI) that has everything pre-installed for us. Select the \u201cDeep Learning Base AMI (Ubuntu 18.04) Version 42.0\u201d from the options returned. You may choose any newer image accordingly that matches with the CUDA and CuDNN library versions used in the base image mentioned in our docker file. Step4: In the next screen choose the instance type. Select \u201cg4dn\u201d and select g4dn.xlarge from the options and proceed. Note that if you have not previously used this GPU, you need to raise a limit increase request in AWS to allocate this GPU to your account\u2019s region. Why G4 instance type? It is the most cost-effective option among the AWS Instances available. Read more about choosing the right GPU on AWS here. Then click on \u201cConfigure Instance details.\u201d Step5: Keep everything as default and click on \u201cAdd Storage\u201d. In the \u201cAdd Storage\u201d section, remember to change the Root Volume size to something like 120 (GiB), etc. We need this as our docker image and other necessary configurations need a good amount of space. Step 6: Next click on \u201cAdd Tags\u201d. Nothing to change here.Next click on \u201cConfigure security group\u201d and click on \u201cAdd Rule\u201d to add custom TCP of port 80 and the source can be from anywhere. This will enable us to host the API on the default port 80 and access the API from anywhere. Step7: Next click on \u201cReview and Launch\u201d and click on \u201cLaunch\u201d. Create a \u201cNew Key Pair\u201d or use an existing Key pair. This is used to SSH and connect to the EC2 machine via tools like Putty if you are on Windows or from the command line. Step8: Once the EC2 machine is ready you can connect to it via any SSH client. Step 9: Once you SSH into the EC2 machine run \u201cgit clone https://github.com/ramsrigouthamg/GPU_Docker_Deployment_HuggingFace_Summarization.git\u201d to clone the repository that contains the docker files to containerize the summarization algorithm from HuggingFace. Step 10: CD into the folder \u201cGPU_Docker_Deployment_HuggingFace_Summarization\u201d and run the following commands one after the other. Here were are downloading the summarization model from HuggingFace locally and packing it within our docker container than downloading it every time with code inside the container. Step 11: Once you are done with the previous step, build the docker image with the following command - It will take some time to built the image, so be patient. Step 12: Once the image is built, run the container with the following command \u2014 Once everything is ready just visit the base URL for the EC2 machine in your browser and you should see a \u201chello world\u201d message. Eg: ec2\u201318\u2013189\u201332\u20138.us-east-2.compute.amazonaws.com Next test the summarization API via tools like Postman. You can pass in any text and get back the summarized version of it. Remember to pass text as well as min_length and max_length parameters. If you want to run the API indefinitely, use console applications like Screen so that the API keeps running even if you close the connection. The main code that converts our Colab notebook shown at the beginning to an API hosted with FastAPI is shown below - And the Dockerfile that is used to create GPU docker from the base Nvidia image is shown below - And here is the requirements.txt file - Point to remember: If you want to be certain, make sure that the drivers (CUDA and CuDNN versions) in the docker image (NVIDIA container toolkit base image) that you created match (or are compatible) with the GPU drivers in the host machine where the container image is run. In AWS you can choose a host machine as an AMI that is compatible with the NVIDIA container toolkit image (remember the usage of FROM nvidia/cuda:11.0-cudnn8-runtime-ubuntu18.04 in your Dockerfile) that you used to create the docker image. You can use a GPU compatible Amazon Machine Image (AMI) as the host image for deploying your API scalably on container orchestration services like ECS etc Happy coding! If you would like to stay connected please feel free to follow me on Twitter and Linkedin", "practical-ai-using-nlp-word-vectors-in-a-novel-way-to-solve-the-problem-of-localization-9de3e4fbf56f": "King \u2014 Man + Woman = Queen You might have seen the traditional word2vec or Glove word embeddings examples that show King -Man+Woman = Queen. Here Queen will be returned from the word embedding algorithm given the words King, Man, and Woman. Today we will see how we can use this structure to solve a real-world problem. An edtech company in the USA wants to expand into India after being successful in its home market. It has a large set of questions in their question bank that it wants to use when it enters the Indian market. But there is one big problem. A sample third class (grade) math question in their question bank looks like this \u2014 Frank lives in San Francisco and Elizabeth lives in Los Angeles. If the flight time is 2 hrs when will Elizabeth reach Frank if she starts at 8am in the morning? A 3rd-grade kid living in India would not connect with this question as it has references to names and locations lesser know to him/her - Frank, San Franciso, Los Angeles, etc. So it would be ideal if we change the question to suit the Indian context and rephrase it\u2014 Sanjay Verma lives in Bangalore and Rekha lives in Mumbai. If the flight time is 2 hrs when will Rekha reach Sanjay Verma if she starts at 8am in the morning? This concept is called localization. It is the general concept of adopting a product or idea to a different country or region respecting local norms, customs, and any other preferences. The goal is to resonate with the target audience for whom the content is localized. Now let\u2019s look at how we can localize our original USA math question to the Indian context. Frank lives in San Francisco and Elizabeth lives in Los Angeles. If the flight time is 2 hrs when will Elizabeth reach Frank if she starts at 8am in the morning? Step 2.1: Our goal is to extract all the keywords that need to be localized. We will use the Spacy Named Entity Recognition to achieve this. Step 2.2: Filter named entities that are irrelevant. For example entities like numbers (cardinal) and time doesn\u2019t need localization in our case. Filtered entities: Frank, San Franciso, Elizabeth, Los Angeles Step 2.3: Now comes the most interesting part. We will use the King-Man + Woman = Queen framework to convert each of the entities. The code is present in the next sections but here we only show the concept. Step 2.4: We go back and change the entities with their replacements to get - Sanjay Verma lives in Bangalore and Rekha lives in Mumbai. If the flight time is 2 hrs when will Rekha reach Sanjay Verma if she starts at 8am in the morning? Google Colaboratory colab.research.google.com Check out the complete and clean Google Colab notebook, that shows two different localization examples. The first example is automated and the second one has simple UI to choose the best replacement manually. The important parts are shown here in code again (besides Colab)\u2014 Step 3.1 Extract entities that need to be localized The output from the above step is \u2014 Step 3.2 Initialize the Google news word vectors from Gensim and perform localization The output from the above step is \u2014 You can see each word along with its top replacement choices. Step 3.3 Print the output with its replacement The output is \u2014 Great! We are finally near the finish line. But what if the first choice shown is not the correct replacement for a given word? To fix that problem, we built a small UI to select the correct choice with a dropdown. It is shown in the Google Colab notebook under example 2. This project is carried out by the awesome intern Niharika Reddy, under my mentorship as a part of my open-source initiative Questgen.ai I launched a very interesting Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it, here is the link. For any questions or if you just wanted to say hi, please reach out to me on Linkedin", "sentence2mcq-using-bert-word-sense-disambiguation-and-t5-transformer-e6bb5aaba29b": "Imagine a middle school English teacher preparing a reading comprehension quiz for the next day\u2019s class. Instead of giving an outdated assessment, the teacher can quickly generate some assessments (MCQs) based on the trending news articles from that day. A human-in-the-loop pipeline can look like this where the news article is summarized into few important sentences and a keyword is taken from each sentence to form an MCQ question. The teacher can then accept or reject the auto-generated questions using NLP to keep only the high-quality ones for the quiz. This saves significant time for the teacher in preparing the assessment as well as increases engagement from the students as it based on currently trending topics. The input to our program will be any sentence with a keyword highlighted (eg: cricket) around which the question needs to be framed. Example input 1: Example input 2 : We will convert any given sentence into an MCQ as shown below - Output for example input 1: Output for example input 2: Note here that we are able to intelligently disambiguate the word cricket as a game in the first sentence and as an insect in the second sentence and generate distractors (wrong MCQs) accordingly. Given our input sentence eg: Mark\u2019s favorite game is **cricket**, we extract the keyword (cricket) and sentence and give it to a T5 transformer algorithm that is trained to take a context and answer as input and generate a question. Parallelly we pass the sentence (Mark\u2019s favorite game is **cricket**) with the highlighted keyword as input to BERT-based algorithm that is trained to do word sense disambiguation (WSD) and identify the correct sense from Wordnet. If we have a sentence \u201c The bat flew into the jungle and landed on a tree\u201d and a keyword \u201cbat\u201d, we automatically know that here we are talking about the mammal bat that has wings, not a cricket bat or baseball bat. Although we humans are good at it, the algorithms are not very good at distinguishing one from the other. The idea of using algorithms to distinguish the exact contextual meaning of a given word is called word sense disambiguation (WSD). WordNet\u00ae is a large lexical database of English. In Wordnet \u201cbat\u201d has several senses (contextual meanings) one for a cricket bat, one for a flying mammal, etc. Among all the several possible senses (contextual meanings) for a given word, our goal is to find the correct sense for a given word in our sentence. We are going to use the awesome BERT-WSD project that is pre-trained to identify the correct sense of a given word in a sentence. Given a sentence with keyword highlighted like (He caught a [TGT] bass [TGT] yesterday), BERT-WSD will score and sort all the possible senses from Wordnet as shown below. We use the sense with the highest probability and extract distractors. Generating distractors using Wordnet Let\u2019s say we get a word like \u201cRed\u201d and identify its sense, we then go to its hypernym using Wordnet. A hypernym is a higher level category for a given word. In our example, color is the hypernym for Red. Then we go to find all hyponyms (sub-categories) of color which might be Purple, Blue, Green, etc all belonging to the color group. So we can use Purple, Blue, Green, as distractors (wrong answer choices) for the given MCQ which has Red as the correct answer. Purple, Blue, Green are also called as Co-Hyponyms of Red. So using Wordnet, extracting co-hyponyms of a given word gets us the distractors of that word. So in the image below, the distractors of Red are Blue, Green, and Purple. Similarly, for our example, we extract the co-hyponyms of the word Cricket to get Football, Polo, Ultimate Frisbee, etc as distractors. Google Colab Notebook An easy to use Google Colab notebook with all the necessary code is available here. Install the necessary libraries : Connect your personal google drive to store the trained BERT-WSD model Download pre-trained BERT WSD model from here Click the download button at the top left of the link to download a file named \u201cbert_base-augmented-batch_size=128-lr=2e-5-max_gloss=6.zip\u201d. Place the zip file in your Google drive home folder. Unzip the above file in your Google drive using the code below. Initialize the BERT-WSD model Create preprocessing steps for BERT-WSD, initialize T5 transformer for question generation and define Wordnet distractor extraction functions. Run the get MCQs function with a given sentence with the highlighted keyword as input. The output from above is - As you can see we took an input sentence (Mark's favorite game is **Cricket**.) and correctly generated the question and relevant distractors for the correct answer (cricket). Now let\u2019s use Gradio and build a nice GUI to do the same. Within the Colab notebook itself, you get a nice GUI to enter any text and see the output right there. Sometimes you don\u2019t get any distractors if the word is not present in Wordnet. If you want to find alternate ways to find distractors find my detailed blog here. What\u2019s Gradio? With Gradio you can create easy to use GUIs for your ML models with just a few lines of code within your colab notebook. And the best part? You can share the GUI via a simple link (eg: 12345.gradio.app) to your non-technical manager/friend to try it out from anywhere on the web. Gradio is not just for text. You can also have an image upload as input and do any predictions in it. I launched a very interesting Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it here is the link. Hope you enjoyed how we solved a real-world problem of generating MCQ from a given sentence using NLP. If you liked this and would like to know more about the \u201cQuestion generation using NLP\u201d course that I am launching soon, please find more information here. If you would like to get updates on more practical AI projects feel free to follow me on Linkedin or Twitter. Happy Learning!", "beyond-tags-and-entering-the-semantic-search-era-on-images-with-openai-clip-1f7d629a9978": "Imagine that you are a writer and you are searching for the best image that goes with your blog or book. You have a search phrase in mind like \u201cTiger playing in the snow\u201d. You go onto copyright-free image websites like Pixabay or Unsplash and try out various combinations of keywords like \u201cTiger\u201d, \u201cSnow\u201d, \u201cTiger Snow\u201d etc to find relevant images. If you are lucky you find the exact image that you are looking for on the first page or in the top N retrieved results. Since the images in these websites have only tags, you are limited by the results retrieved by tags and you need human supervision to further filter the most relevant images for your search phrase \u201cTiger playing in the snow\u201d. This is a problem if you are looking to find relevant images for your search phrases frequently and it is time-consuming to further filter on the images retrieved from tags. What you need is a semantic search on top of the retrieved images with tags. Semantic search refers to the ability of search engines to consider the intent and contextual meaning of search phrases. Instead of trying to find exact matches for the word in the input phrase, semantic search captures broader context and relationships between words and retrieves results that are more closely related to the context of the search query. If each image in the Pixabay/Unsplash library is captioned with a sentence describing it, then we can use Sentence-BERT or other similar algorithms to encode the sentence to a vector and perform similarity to extract the top N captions and relevant images that match semantically to the search query. But it is expensive to caption every image in the database. The best we have is just tags most of the time. OpenAI CLIP comes to our rescue. In text search, if you ask for a question like \u201cWho is the 45th president of the United States?\u201d and if you have millions of documents to look for the answer, you first do a lightweight filter (Eg; BM25 in Elastic Search) that can quickly go through all the million documents and get top 100 or so documents that might have the answer. Then you use a heavier algorithm (eg: Sentence -BERT) that can do the semantic search to extract the exact paragraph that has the answer among these 100 filtered documents. For a given search query, first, we have a lightweight fast filter to find potential candidate documents. We call this Retriever. The goal of the retriever is sifting out the obvious negative cases. Then we have Reader that uses computationally heavier algorithms (eg: Transformer based) to narrow down to the exact region in the document that has the answer using semantic search. Applying a similar analogy to the image search, we see that retrieving images for a given search query based on Tags is extremely fast and lightweight. This is the Retriever part. For example, you retrieve all the images of Tiger from Pixabay by just searching with the tag \u201cTiger\u201d. Then we have OpenAI CLIP as our Reader that uses computationally heavier algorithms to encode each of our retrieved images from tags to compare with our encoded sentence eg: \u201cTiger playing in the snow\u201d and return the top N results based on semantic similarity. OpenAI recently (Jan 5, 2021) introduced a neural network called CLIP which efficiently learns visual concepts from natural language supervision. CLIP stands for Contrastive Language\u2013Image Pre-training. CLIP builds on a large body of work on zero-shot transfer, natural language supervision, and multimodal learning. You can read more about CLIP here. For our use-case, we need to understand that CLIP is trained with a large set of images with their corresponding captions so it learned to predict which image caption (text) matches closely with which image when a similarity metric (eg: Cosine) is applied on encodings of both image and text. So after training, you can give a random image and find cosine similarity of that image in the vector space with two vectors of phrases \u201cIs this photo of a dog?\u201d, \u201cIs this photo of a cat?\u201d and see which one has the highest similarity to find the class of the image. So in a way, it has zero-shot classification capabilities like GPT-2 and GPT-3. Enough talk show me the code \ud83d\ude03 An easy to use Google Colab notebook for this blog is available here. I adapted the code from the OpenAI CLIP team\u2019s original Github repo. I will focus on the code parts that I changed for this use-case. 1. Retrieve images for a given tag using Pixabay First, we use the Pixabay API to retrieve images for a given search Tag. Eg: Tiger and display the retrieved images using an easy to use plotting library ipyplot. Retrieved images with tag: Tiger from Pixabay 2. Functions to preprocess the images and text In the following code, we preprocess the images and text, encode them and convert them into fixed-sized vectors with the model. We also have a function to find cosine similarity between the encoded text phrase eg: \u201cTiger playing the snow\u201d and vectors of all the retrieved images. Then we sort the images based on the similarity to get the top N similar images to the text phrase. 3. Search with any phrase and get the top N semantically similar images In this section, the code below takes any search phrase like \u201cTiger playing in the snow\u201d, encodes the text into a vector, and calculates cosine similarity with all the image vectors, and filters the top N choices and displays. Output: We get back the top N images that are most similar to our search phrase \u201cTiger playing in the snow\u201d, after filtering through the Pixabay images retrieved from the tag (Tiger). Similarly searching with the phrase \u201cTiger immersed in water\u201d retrieves these images shown below. One more : Search Phrase \u2014 \u201cTiger sitting on a branch\u201d I launched a very interesting Udemy course titled \u201cQuestion generation using NLP\u201d expanding on some of the techniques discussed in this blog post. If you would like to take a look at it, here is the link. Hope you enjoyed how we put cutting-edge research from OpenAI that just got released 2 days ago to use and solved a real-world problem. If you would like to get updates on more practical AI projects feel free to follow me on Linkedin or Twitter. Happy Learning!", "practical-tips-for-beginners-in-data-science-debunking-few-myths-30537117a4e4": "When I started my journey in 2015 to where I am right now, there has been a lot of learning through professional experience, MOOCs, and online communities. I think it\u2019s time to start giving back to the Data Science world, and I begin my blog series with something I felt necessary to talk about. This post, in particular, is aimed at people who are beginning their Data Science career (or switching to it) and are biased/not-known with some of the facts related to the professional experience. Most people become fancied by applying machine learning algorithms to the already given raw (not-so-raw actually) dataset with a clear problem statement. They usually sound like this \u2014 \u201cCompany XYZ wants to automate their loan approval process. Using the given training data, can you identify the important factors to classify and predict the loan approvals. Note that the metric you will be evaluated on is Accuracy\u201d. These problem statements unknowingly simplify certain things like defining the problem, attaining the final dataset, identifying the evaluation metric, etc. But in the real world, this is not the case. \u201cRemember, more than 60% of your time is spend in identifying/defining the problem, brainstorming the approach and getting the relevant data.\u201d To kick start on this problem-solving \u2014 find open-source data sets, come up with your hypothesis, ask yourself what problems can this data set solve, think about what other relevant data you might use to solve this. You might end up solving your problem using a t-test with some beautiful visualizations :) SQL is to Data Science what Gautam Gambhir was to Indian Cricket. You will struggle during the innings if you don\u2019t get a good start, and yet, sometimes, you will be forgotten by the time of victory sixers. After all, SQL is used to fetch some data. But imagine, you did the entire modeling exercise on the customer level dataset to realize that the data has customer duplicates because of a few wrong \u2018SQL joins\u2019 during data wrangling. \u2018Duplication\u2019 is one major problem while data fetching, and this might be because of your SQL code or because of issues in the backend database. Thorough quality assessment is required after every step of SQL data-wrangling because \u201cexecuting without syntax error doesn\u2019t mean it executed the way you wanted it to be.\u201d For beginners, most of the resources teach the basic SQL with concentration on getting the syntax part, and the notion becomes spread that SQL logic is so simple and often gets overlooked. I would suggest you take a relational database (set of 3\u201310 tables) on different topics (Airlines, E-commerce, retail, etc.) and ask some tough business questions. This will require the execution of several SQL blocks and will adequately test your skills. There are already numerous amount of blogs on this topic, and if you read some of them, Python would be the winner (by a small margin). I don\u2019t want to go into the pros and cons of each and decide something, but, I would suggest you to learn both and get expertise in one. Let\u2019s say, Mr. RaceR was introduced to R. During his journey, he loved the intuitiveness of dplyr, he felt that ggplot2 is the next beautiful thing after his girlfriend and his one-liner fancy decision tree plot works like a charm. And suddenly, one day, he got an interview call from his dream company with an excellent opportunity. But the only heartbreaking thing he got to know was that people there find pandas cute, they generate reports over matplotlib and run Jupyter notebook based deep learning models on cloud GPUs. He felt his whole R life was a lie.Although I dramatized the story, the moral is to learn and get comfortable in both up to the extent that switching becomes quite easy (It might be because of a job change, client requirements, run-time data issues, better package/modules availability, etc). After all, languages are just the tools to get things done. You never know that there is a third language waiting to break the storm.FYI \u2014 I love R as I started with it :) I suggest this because of two main reasons. One \u2014 You should enjoy exploring unseen data. Structured data would be the best way to start, and the process involves bi-variate analysis, testing a few hypotheses, missing value treatment, outlier identification, etc. Two \u2014 You will go through the iterative process of variable selection, feature engineering, parameter tuning, and finally checking for business implications of model variables. On the other hand, in deep learning, network architecture takes care of most of the feature engineering part, and hyper-parameter tuning becomes an essential part of the process. Also, explaining the model to the business stakeholders is an integral part of the project, and hence, the intuitiveness of the ML models in terms of variable importance (like logistic regression, decision trees) make them the most sought out option (for structured data) in many industries. Yes, reread it. The art of Data Science doesn\u2019t involve building the best model, but in solving the problem using data as an asset There are lot of other traditional methodologies and techniques which are widely used in the industry. Dashboarding and Reporting \u2014 These are an integral part of many businesses and involves creating automated data pipelines using SQL, brainstorming with stakeholders to understand KPIs, storytelling with a strong visual aesthetic narrative Design of Experiments \u2014 It is always essential to know how specific services/campaigns/promotions worked, and measuring their impact becomes crucial to make key decisions. Test-control analysis, A/B testing will be generally used here to quantify that impact Segment the data till you can \u2014 Certain problems can quickly be answered with properly designed EDA and segmentation For beginners, I would suggest you to choose areas of interest and find relevant datasets (Movies, Sports, etc). Create Tableau/RShiny public dashboards keeping two things in mind. How well the information is flowing and how easily your chart is depicting that information. Also, read case studies where the emphasis is on designing and formulating solutions to certain problems. Yes, you will, at some point in time, get lost in the digital learning world of Data Science. This doesn\u2019t mean you have to complete all of them to become an expert. To get rid of that bias and have a continuous learning experience, you should create your learning paths and set principles based on your learning style and motivation. First, know what kind of approach you want to take; top-down or bottom-up. I am a former kind of person who would like to know a sufficient amount of theoretical knowledge before jumping into applying them, but it strictly depends on individualPrepare a learning track based on the objective and the above approach chosenDo one thing at a time and COMPLETE IT. Let it be a course on Coursera, project on Kaggle or creating your tableau dashboardMake your own notes of every learningKeep an aim to read one blog a day. Note down two things that you can take home with youFinally, consistency is the key Thank you so much for reading until the end. Please hit clap if you like, and if you have something to say, drop a comment below. It will help me refine my posts :) This post is based on my professional experience until now. If you have any feedback or major additions you would like to point out for beginners, please feel free to comment. Connect with me on LinkedIn: www.linkedin.com/in/muralimohanakrishnadandu"}